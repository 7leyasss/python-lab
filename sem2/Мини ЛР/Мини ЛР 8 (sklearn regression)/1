import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
import random

def f(x):
    return np.sin(x) + 0.5 * x - 0.1 * x**2

print("Функция является нелинейной (содержит sin(x) и x^2)")

models = [
    ('Linear Regression', LinearRegression()),
    ('Support Vector Regression', SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)),
    ('Gradient Boosting', GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3))
]

x_min, x_max = -10, 10
x = np.linspace(x_min, x_max, 100).reshape(-1, 1)
y_true = f(x)
noise = np.array([random.uniform(-1, 1) for _ in range(100)]).reshape(-1, 1)
y = y_true + noise

plt.figure(figsize=(15, 10))

for i, (name, model) in enumerate(models):
    model.fit(x, y.ravel())
    y_pred = model.predict(x)
    
    mse = mean_squared_error(y_true, y_pred)
    
    plt.subplot(3, 1, i+1)
    plt.scatter(x, y, color='blue', label='Исходные точки с шумом', alpha=0.6)
    plt.plot(x, y_true, color='green', linewidth=2, label='Истинная функция')
    plt.plot(x, y_pred, color='red', linewidth=2, label=f'Предсказание ({name})')
    plt.title(f'{name}\nMSE: {mse:.4f}')
    plt.legend()
    plt.grid(True)

plt.tight_layout()
plt.show()

"""
Выводы
1. Linear Regression: Хорошо работает для линейных компонентов функции, но не может уловить нелинейные зависимости
2. SVR: Лучше справляется с нелинейностями благодаря ядру RBF, но требует тонкой настройки параметров
3. Gradient Boosting: Наилучший результат благодаря способности моделировать сложные нелинейные зависимости
"""
